Objectives
Use Linear Regression in one variable to fit the parameters to a model
Use Linear Regression in multiple variables to fit the parameters to a model
Use Polynomial Regression in single variable tofit the parameters to a model
Create a pipeline for performing linear regression using multiple features in polynomial scaling
Evaluate the performance of different forms of regression on basis of MSE and R^2 parameters
Setup
we are going to make use of following libraries:

pandas for managing the data.
numpy for mathematical operations.
sklearn for machine learning and machine-learning-pipeline related functions.
seaborn for visualizing the data.
matplotlib for additional plotting tools.
Importing Required Libraries
All the required libraries are being imported (here):

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings("ignore", category=UserWarning) 
%matplotlib inline
Importing the dataset
Run the cell below to download the dataset.

This function will download the dataset into your browser

#This function will download the dataset into your browser 
import requests
import os

def download_data(url, filename):
    """
    Downloads a file from a URL using the standard 'requests' library.
    """
    try:
        print(f"Attempting to download {url}...")
        response = requests.get(url, stream=True)
        
        # Raise an exception for bad status codes (4xx or 5xx)
        response.raise_for_status() 
        
        with open(filename, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)
        
        print(f"Successfully downloaded to {os.path.abspath(filename)}")
        
    except requests.exceptions.RequestException as e:
        print(f"Error during download: {e}")
path = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-Coursera/laptop_pricing_dataset_mod2.csv"
Downloading the dataset using the download() function:

# download the dataset;
download_data(path, "laptops.csv")
file_name="laptops.csv"
Attempting to download https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-Coursera/laptop_pricing_dataset_mod2.csv...
Successfully downloaded to C:\Users\Shreyash\Downloads\laptops.csv
Load the dataset into a pandas dataframe

df = pd.read_csv(file_name, header=0)
# show the first 5 rows using dataframe.head() method
print("The first 5 rows of the dataframe") 
df.head(5)
The first 5 rows of the dataframe
Unnamed: 0.1	Unnamed: 0	Manufacturer	Category	GPU	OS	CPU_core	Screen_Size_inch	CPU_frequency	RAM_GB	Storage_GB_SSD	Weight_pounds	Price	Price-binned	Screen-Full_HD	Screen-IPS_panel
0	0	0	Acer	4	2	1	5	14.0	0.551724	8	256	3.52800	978	Low	0	1
1	1	1	Dell	3	1	1	3	15.6	0.689655	4	256	4.85100	634	Low	1	0
2	2	2	Dell	3	1	1	7	15.6	0.931034	8	256	4.85100	946	Low	1	0
3	3	3	Dell	4	2	1	5	13.3	0.551724	8	128	2.69010	1244	Low	0	1
4	4	4	HP	4	2	1	7	15.6	0.620690	8	256	4.21155	837	Low	1	0
Single Linear Regression
Make a linear regression model using CPU frequency as the only input to predict the price.

# Single Linear Regression
lm = LinearRegression()

X = df[['CPU_frequency']]
Y = df['Price']

lm.fit(X,Y)

Yhat=lm.predict(X)
For checking model performance generate the Distribution plot for the predicted values and that of the actual values.

# checking model performance
ax1 = sns.distplot(df['Price'], hist=False, color="r", label="Actual Value")

# Create a distribution plot for predicted values
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values" , ax=ax1)

plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price')
plt.ylabel('Proportion of laptops')
plt.legend(['Actual Value', 'Predicted Value'])
plt.show()

Evaluating the Mean Squared Error and R^2 score values for the model.

# Evaluating the Mean Squared Error and R^2 score values for the model.
mse_slr = mean_squared_error(df['Price'], Yhat)
r2_score_slr = lm.score(X, Y)
print('The R-square for Linear Regression is: ', r2_score_slr)
print('The mean square error of price and predicted value is: ', mse_slr)
The R-square for Linear Regression is:  0.1344436321024326
The mean square error of price and predicted value is:  284583.4405868629
Multiple Linear Regression
Use all these variables to create a Multiple Linear Regression system.

#  Multiple Linear Regression
lm1 = LinearRegression()
Z = df[['CPU_frequency','RAM_GB','Storage_GB_SSD','CPU_core','OS','GPU','Category']]
lm1.fit(Z,Y)
Y_hat = lm1.predict(Z)
Plot the Distribution graph of the predicted values as well as the Actual values

# Plotting the Distribution graph
ax1 = sns.distplot(df['Price'], hist=False, color="r", label="Actual Value")
sns.distplot(Y_hat, hist=False, color="b", label="Fitted Values" , ax=ax1)

plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price')
plt.ylabel('Proportion of laptops')
Text(0, 0.5, 'Proportion of laptops')
Find the R^2 score and the MSE value for this fit.

# Calculate R-squared (R^2) score
r_squared = r2_score(Y, Y_hat)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(Y, Y_hat)

# Calculate Root Mean Squared Error (RMSE) for better interpretability
rmse = np.sqrt(mse)

print("--- Model Performance Metrics ---")
print(f"R-squared (R^2) Score: {r_squared:.4f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
--- Model Performance Metrics ---
R-squared (R^2) Score: 0.5083
Mean Squared Error (MSE): 161680.57
Root Mean Squared Error (RMSE): 402.10
Polynomial Regression
Use the variable "CPU_frequency" to create Polynomial features.

#  Polynomial Regression
X = X.to_numpy().flatten()
f1 = np.polyfit(X, Y, 1)
p1 = np.poly1d(f1)

f3 = np.polyfit(X, Y, 3)
p3 = np.poly1d(f3)

f5 = np.polyfit(X, Y, 5)
p5 = np.poly1d(f5)
Draw a graph showing the model's predictions and the real data to see how well the model fits.

def PlotPolly(model, independent_variable, dependent_variabble, Name):
    x_new = np.linspace(independent_variable.min(),independent_variable.max(),100)
    y_new = model(x_new)

    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')
    plt.title(f'Polynomial Fit for Price ~ {Name}')
    ax = plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig = plt.gcf()
    plt.xlabel(Name)
    plt.ylabel('Price of laptops')
Call this function for the 3 models created and get the required graphs.

# Call for function of degree 1
PlotPolly(p1, X, Y, 'CPU_frequency')
# Call for function of degree 3
PlotPolly(p3, X, Y, 'CPU_frequency')
# Call for function of degree 5
PlotPolly(p5, X, Y, 'CPU_frequency')
Calculate the R^2 and MSE values for these fits.

# Calculating R^2 and MSE values
r_squared_1 = r2_score(Y, p1(X))
print('The R-square value for 1st degree polynomial is: ', r_squared_1)
print('The MSE value for 1st degree polynomial is: ', mean_squared_error(Y,p1(X)))
r_squared_3 = r2_score(Y, p3(X))
print('The R-square value for 3rd degree polynomial is: ', r_squared_3)
print('The MSE value for 3rd degree polynomial is: ', mean_squared_error(Y,p3(X)))
r_squared_5 = r2_score(Y, p5(X))
print('The R-square value for 5th degree polynomial is: ', r_squared_5)
print('The MSE value for 5th degree polynomial is: ', mean_squared_error(Y,p5(X)))
The R-square value for 1st degree polynomial is:  0.1344436321024326
The MSE value for 1st degree polynomial is:  284583.4405868629
The R-square value for 3rd degree polynomial is:  0.26692640796530986
The MSE value for 3rd degree polynomial is:  241024.8630384881
The R-square value for 5th degree polynomial is:  0.3030822706444306
The MSE value for 5th degree polynomial is:  229137.29548052172
Pipeline
Creating a pipeline which performs parameter scaling, Polynomial Feature generation and Linear regression using the set of multiple features .

# Pipeline  
Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]
pipe=Pipeline(Input)
Z = Z.astype(float)
pipe.fit(Z,Y)
ypipe=pipe.predict(Z)
Evaluate the MSE and R^2 values for the this predicted output.

#  Evaluating MSE and R^2 values
print('MSE for multi-variable polynomial pipeline is: ', mean_squared_error(Y, ypipe))
print('R^2 for multi-variable polynomial pipeline is: ', r2_score(Y, ypipe))
MSE for multi-variable polynomial pipeline is:  120595.8612802837
R^2 for multi-variable polynomial pipeline is:  0.6332094535859659
You should notice that the R² value gets higher when we move from single linear regression to multiple linear regression. If we also add polynomial features to the multiple linear regression, the R² value improves even more.

Price Prediction
